apiVersion: ray.io/v1
kind: RayService
metadata:
  name: "${{values.name}}-${{values.namespace}}"
  namespace: "${{values.namespace}}"
  labels:
    app: "${{values.name}}-${{values.namespace}}"
    backstage.io/kubernetes-id: "${{values.name}}-${{values.namespace}}"
    entity-id: "${{values.name}}-${{values.namespace}}"
spec:
  deploymentUnhealthySecondThreshold: 300
  rayClusterConfig:
    enableInTreeAutoscaling: true
    headGroupSpec:
      headService:
        metadata:
          name: "${{values.name}}"
          namespace: "${{values.namespace}}"
      rayStartParams:
        dashboard-host: 0.0.0.0
        num-cpus: '0'
      template:
        metadata:
          labels:
            app: "${{values.name}}-${{values.namespace}}"
            backstage.io/kubernetes-id: "${{values.name}}-${{values.namespace}}"
            entity-id: "${{values.name}}-${{values.namespace}}"
        spec:
          containers:
            - env:
                - name: RAY_GRAFANA_HOST
                  value: http://prometheus-grafana.monitoring.svc.cluster.local
                - name: RAY_PROMETHEUS_HOST
                  value: >-
                    http://prometheus-kube-prometheus-prometheus.monitoring.svc.cluster.local:9090
                - name: RAY_GRAFANA_IFRAME_HOST
                  value: https://grafana.arcadeai.site
              image: public.ecr.aws/data-on-eks/ray-serve-inf2-llama2:latest
              imagePullPolicy: IfNotPresent
              lifecycle:
                preStop:
                  exec:
                    command:
                      - /bin/sh
                      - '-c'
                      - ray stop
              name: ray-head
              ports:
                - containerPort: 6379
                  name: gcs
                  protocol: TCP
                - containerPort: 8265
                  name: dashboard
                  protocol: TCP
                - containerPort: 10001
                  name: client
                  protocol: TCP
                - containerPort: 8000
                  name: serve
                  protocol: TCP
              resources:
                limits:
                  cpu: '1'
                  memory: 6G
                requests:
                  cpu: '1'
                  memory: 6G
              volumeMounts:
                - mountPath: /tmp/ray
                  name: ray-logs
          volumes:
            - emptyDir: {}
              name: ray-logs
    rayVersion: 2.7.1
    workerGroupSpecs:
      - groupName: inf2-worker-group
        maxReplicas: ${{values.max_replicas}}
        minReplicas: ${{values.min_replicas}}
        numOfHosts: 1
        rayStartParams: {}
        replicas: ${{values.starting_replicas}}
        template:
          metadata:
            labels:
              app: "${{values.name}}-${{values.namespace}}"
              backstage.io/kubernetes-id: "${{values.name}}-${{values.namespace}}"
              entity-id: "${{values.name}}-${{values.namespace}}"
          spec:
            containers:
              - image: public.ecr.aws/data-on-eks/ray-serve-inf2-llama2:latest
                imagePullPolicy: IfNotPresent
                lifecycle:
                  preStop:
                    exec:
                      command:
                        - /bin/sh
                        - '-c'
                        - ray stop
                name: ray-worker
                resources:
                  limits:
                    aws.amazon.com/neuron: '12'
                    cpu: '180'
                    memory: 700G
                  requests:
                    aws.amazon.com/neuron: '12'
                    cpu: '180'
                    memory: 700G
            tolerations:
              - effect: NoSchedule
                key: aws.amazon.com/neuron
                operator: Exists
  serveConfigV2: |
    applications:
      - name: llama2
        import_path: ray_serve_llama2:entrypoint # Specify the correct path to your Python script
        runtimeEnv: 
          env_vars: {"MODEL_ID": "NousResearch/Llama-2-13b-chat-hf"}  # Replace with the appropriate model ID
        deployments:
          - name: LlamaModel
            autoscaling_config:
              metrics_interval_s: 0.2
              min_replicas: ${{values.min_replicas}}
              max_replicas: ${{values.max_replicas}}
              look_back_period_s: 2
              downscale_delay_s: 5
              upscale_delay_s: 2
              target_num_ongoing_requests_per_replica: 1
  serviceUnhealthySecondThreshold: 900
